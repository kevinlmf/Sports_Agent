# Multimodal Vision-Language Model Training Configuration
# Sports Injury Risk Prediction System

# ========================================
# MODEL CONFIGURATION
# ========================================
model:
  # Model architecture
  tabular_input_dim: 50  # Number of tabular features
  vision_encoder: 'clip'  # Options: 'clip', 'vit', 'resnet'
  text_encoder: 'bert'    # Options: 'bert', 'biobert', 'clip'
  embed_dim: 768          # Embedding dimension for all modalities
  n_heads: 8              # Number of attention heads in cross-modal fusion
  dropout: 0.1            # Dropout rate

  # Task configuration
  task: 'classification'  # Options: 'classification', 'survival'
  num_classes: 2          # Number of output classes (for classification)

  # Freezing configuration
  freeze_vision: true     # Freeze vision encoder weights
  freeze_text: true       # Freeze text encoder weights

  # LoRA configuration
  use_lora: true          # Enable LoRA adapters
  lora_r: 16              # LoRA rank (lower = fewer parameters)
  lora_alpha: 32          # LoRA scaling parameter
  lora_dropout: 0.1       # LoRA dropout
  target_modules:         # Which modules to apply LoRA to
    - "query"
    - "value"

# ========================================
# DATA CONFIGURATION
# ========================================
data:
  # Paths
  train_tabular: 'data/train.csv'
  val_tabular: 'data/val.csv'
  test_tabular: 'data/test.csv'

  train_text: 'data/train_notes.csv'
  val_text: 'data/val_notes.csv'
  test_text: 'data/test_notes.csv'

  train_image_dir: 'data/images/train'
  val_image_dir: 'data/images/val'
  test_image_dir: 'data/images/test'

  # Data loading
  batch_size: 32
  num_workers: 0  # MUST be 0 to avoid mutex deadlocks with transformers models
  pin_memory: true

  # Data preprocessing
  max_text_length: 512
  image_size: [224, 224]
  label_column: 'injury'
  id_column: 'athlete_id'

  # Data augmentation
  image_aug_prob: 0.5
  text_aug_prob: 0.3
  tabular_noise_std: 0.05

# ========================================
# TRAINING CONFIGURATION
# ========================================
training:
  # Optimization
  optimizer: 'adamw'      # Options: 'adam', 'adamw', 'sgd'
  learning_rate: 1e-4
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  eps: 1e-8

  # Learning rate scheduling
  scheduler: 'cosine'     # Options: 'cosine', 'step', 'plateau', 'linear'
  warmup_epochs: 5
  min_lr: 1e-6

  # Training loop
  epochs: 100
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0      # Gradient clipping

  # Mixed precision training
  use_amp: true           # Automatic Mixed Precision (FP16)

  # Early stopping
  early_stopping: true
  early_stopping_patience: 10
  early_stopping_metric: 'val_auc'
  early_stopping_mode: 'max'  # 'max' for AUC, 'min' for loss

  # Checkpointing
  save_best_only: true
  checkpoint_dir: 'checkpoints/'
  save_frequency: 5       # Save every N epochs

# ========================================
# LOSS CONFIGURATION
# ========================================
loss:
  # Primary loss
  primary_loss: 'cross_entropy'  # Options: 'cross_entropy', 'focal', 'bce'

  # Class weights (for imbalanced data)
  use_class_weights: false
  class_weights: [1.0, 1.0]

  # Auxiliary losses
  use_contrastive_loss: false
  contrastive_weight: 0.1

  use_reconstruction_loss: false
  reconstruction_weight: 0.05

# ========================================
# KNOWLEDGE DISTILLATION CONFIGURATION
# ========================================
distillation:
  enabled: false
  teacher_checkpoint: 'checkpoints/teacher_model.pth'
  temperature: 3.0
  alpha: 0.7  # Weight for hard target loss (1-alpha for soft)

  # Progressive distillation
  use_progressive: false
  layer_weights: [0.1, 0.2, 0.3, 0.4]

# ========================================
# DISTRIBUTED TRAINING CONFIGURATION
# ========================================
distributed:
  # DeepSpeed
  use_deepspeed: false
  deepspeed_config: 'configs/deepspeed_config.json'

  # Multi-GPU
  use_ddp: false          # Distributed Data Parallel
  world_size: 1
  local_rank: 0

  # Ray Tune (hyperparameter search)
  use_ray_tune: false
  ray_num_samples: 20
  ray_scheduler: 'asha'   # Options: 'asha', 'pbt', 'hyperband'

# ========================================
# EXPERIMENT TRACKING CONFIGURATION
# ========================================
tracking:
  # General
  experiment_name: 'sports_injury_vlm'
  run_name: 'vlm_lora_v1'

  # MLflow
  use_mlflow: true
  mlflow_tracking_uri: 'http://localhost:5000'
  mlflow_experiment_name: 'sports_injury_risk'

  # Weights & Biases
  use_wandb: false
  wandb_project: 'sports_injury_risk'
  wandb_entity: 'your_username'
  wandb_watch_model: true
  wandb_log_frequency: 10

  # Logging
  log_interval: 10        # Log every N batches
  eval_interval: 1        # Evaluate every N epochs
  save_interval: 5        # Save checkpoint every N epochs

# ========================================
# EVALUATION CONFIGURATION
# ========================================
evaluation:
  # Metrics to compute
  metrics:
    - 'auc'
    - 'f1'
    - 'precision'
    - 'recall'
    - 'accuracy'
    - 'confusion_matrix'

  # Test-time augmentation
  use_tta: false
  tta_steps: 5

  # Threshold optimization
  optimize_threshold: true
  threshold_metric: 'f1'

# ========================================
# INTERPRETABILITY CONFIGURATION
# ========================================
interpretability:
  # SHAP
  compute_shap: true
  shap_num_samples: 100
  shap_background_size: 100

  # Grad-CAM
  compute_gradcam: true
  gradcam_target_layer: 'vision_model.layer4'

  # Attention visualization
  visualize_attention: true
  attention_save_dir: 'visualizations/attention'

  # Feature importance
  save_feature_importance: true
  feature_importance_dir: 'results/feature_importance'

# ========================================
# INFERENCE CONFIGURATION
# ========================================
inference:
  # Model loading
  checkpoint_path: 'checkpoints/best_model.pth'

  # Batch inference
  inference_batch_size: 64

  # Export formats
  export_onnx: false
  export_torchscript: false
  onnx_opset_version: 14

  # Deployment
  deploy_api: false
  api_host: '0.0.0.0'
  api_port: 8000

# ========================================
# COMPUTATIONAL RESOURCES
# ========================================
compute:
  # Device
  device: 'cuda'          # Options: 'cuda', 'cpu', 'mps'
  gpu_ids: [0]            # GPU device IDs

  # Memory optimization
  use_gradient_checkpointing: false
  empty_cache_freq: 10    # Empty CUDA cache every N batches

  # Precision
  precision: 'fp16'       # Options: 'fp32', 'fp16', 'bf16'

# ========================================
# REPRODUCIBILITY
# ========================================
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false        # CUDNN benchmark (faster but non-deterministic)

# ========================================
# DEBUGGING
# ========================================
debug:
  enabled: false
  debug_batches: 5        # Only train on first N batches
  verbose: false
  profile_memory: false
  profile_time: false

# ========================================
# CUSTOM CONFIGURATION
# ========================================
custom:
  # Add any custom hyperparameters here
  use_custom_loss: false
  custom_param_1: 0.5
  custom_param_2: 'value'
