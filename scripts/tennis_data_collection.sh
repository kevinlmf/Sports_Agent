#!/bin/bash
################################################################################
# ç½‘çƒè¿åŠ¨å‘˜æ•°æ®æ”¶é›†å’Œåˆ†æå®Œæ•´è„šæœ¬
# Tennis Player Data Collection and Analysis Pipeline
#
# åŠŸèƒ½ï¼š
# 1. ä»å¤šä¸ªæ•°æ®æºæ”¶é›†ç½‘çƒè¿åŠ¨å‘˜æ•°æ®
# 2. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
# 3. ç‰¹å¾å·¥ç¨‹
# 4. ä¼¤ç—…é£é™©é¢„æµ‹æ¨¡å‹è®­ç»ƒ
# 5. ç»“æœå¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ
#
# ä½¿ç”¨æ–¹æ³•ï¼š
#   bash tennis_data_collection.sh [é€‰é¡¹]
#
# é€‰é¡¹ï¼š
#   --collect-only    åªæ”¶é›†æ•°æ®ï¼Œä¸è¿›è¡Œåˆ†æ
#   --analyze-only    åªåˆ†æå·²æœ‰æ•°æ®
#   --full            å®Œæ•´æµç¨‹ï¼ˆé»˜è®¤ï¼‰
################################################################################

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT="/Users/mengfanlong/Downloads/System/MLE/Engine/Sports_Injury_Risk"
cd "$PROJECT_ROOT"

# æ•°æ®ç›®å½•
DATA_DIR="$PROJECT_ROOT/data/tennis"
RAW_DIR="$DATA_DIR/raw"
PROCESSED_DIR="$DATA_DIR/processed"
RESULTS_DIR="$PROJECT_ROOT/results/tennis"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
mkdir -p "$RAW_DIR" "$PROCESSED_DIR" "$RESULTS_DIR"

################################################################################
# å·¥å…·å‡½æ•°
################################################################################

log_info() {
    echo -e "${BLUE}[INFO]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"
}

print_header() {
    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘  $1"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
}

check_dependencies() {
    log_info "æ£€æŸ¥ä¾èµ–..."

    # æ£€æŸ¥Python
    if ! command -v python &> /dev/null; then
        log_error "Pythonæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…Python 3.8+"
        exit 1
    fi

    # æ£€æŸ¥pipåŒ…
    python -c "import pandas, numpy, sklearn, torch" 2>/dev/null || {
        log_warning "ç¼ºå°‘å¿…è¦çš„PythonåŒ…ï¼Œæ­£åœ¨å®‰è£…..."
        pip install -q pandas numpy scikit-learn torch torchvision
    }

    log_success "ä¾èµ–æ£€æŸ¥å®Œæˆ"
}

################################################################################
# æ­¥éª¤1: æ•°æ®æ”¶é›†
################################################################################

collect_data() {
    print_header "æ­¥éª¤1: æ”¶é›†ç½‘çƒè¿åŠ¨å‘˜æ•°æ®"

    # åˆ›å»ºPythonè„šæœ¬æ¥æ”¶é›†æ•°æ®
    cat > "$RAW_DIR/collect_tennis_data.py" << 'PYTHON_SCRIPT'
#!/usr/bin/env python3
"""
ç½‘çƒè¿åŠ¨å‘˜æ•°æ®æ”¶é›†è„šæœ¬
ä»å¤šä¸ªæ•°æ®æºæ”¶é›†ATP/WTAè¿åŠ¨å‘˜çš„æ•°æ®
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import requests
import json
import time
import random

def generate_tennis_player_data(n_players=200):
    """
    ç”Ÿæˆæ¨¡æ‹Ÿçš„ç½‘çƒè¿åŠ¨å‘˜æ•°æ®

    åŒ…å«ï¼š
    - åŸºæœ¬ä¿¡æ¯ï¼šå§“åã€å¹´é¾„ã€æ€§åˆ«ã€èº«é«˜ã€ä½“é‡
    - æ¯”èµ›æ•°æ®ï¼šæ¯”èµ›åœºæ¬¡ã€èƒœç‡ã€æ’å
    - è®­ç»ƒæ•°æ®ï¼šè®­ç»ƒæ—¶é•¿ã€è®­ç»ƒå¼ºåº¦
    - ä¼¤ç—…å†å²ï¼šè¿‡å¾€ä¼¤ç—…è®°å½•
    - ç”Ÿç†æŒ‡æ ‡ï¼šå¿ƒç‡ã€ä½“èƒ½æŒ‡æ•°
    """

    np.random.seed(42)
    random.seed(42)

    print(f"ç”Ÿæˆ {n_players} åç½‘çƒè¿åŠ¨å‘˜çš„æ•°æ®...")

    # å§“ååˆ—è¡¨
    first_names = ['Rafael', 'Roger', 'Novak', 'Andy', 'Dominic', 'Stefanos',
                   'Daniil', 'Alexander', 'Carlos', 'Casper', 'Serena', 'Venus',
                   'Naomi', 'Ashleigh', 'Simona', 'Iga', 'Garbine', 'Karolina']
    last_names = ['Nadal', 'Federer', 'Djokovic', 'Murray', 'Thiem', 'Tsitsipas',
                  'Medvedev', 'Zverev', 'Alcaraz', 'Ruud', 'Williams', 'Osaka',
                  'Barty', 'Halep', 'Swiatek', 'Muguruza', 'Pliskova', 'Kvitova']

    data = []

    for i in range(n_players):
        player = {
            # åŸºæœ¬ä¿¡æ¯
            'player_id': f'P{i+1:04d}',
            'name': f"{random.choice(first_names)} {random.choice(last_names)}",
            'age': np.random.randint(18, 38),
            'gender': np.random.choice(['M', 'F'], p=[0.5, 0.5]),
            'height_cm': np.random.randint(160, 200),
            'weight_kg': np.random.randint(55, 95),
            'country': np.random.choice(['USA', 'Spain', 'Serbia', 'UK', 'France',
                                        'Germany', 'Australia', 'Russia', 'Japan']),

            # èŒä¸šä¿¡æ¯
            'years_pro': np.random.randint(1, 20),
            'current_ranking': i + 1 + np.random.randint(-10, 10),
            'highest_ranking': max(1, i + 1 - np.random.randint(0, 20)),
            'career_prize_money': np.random.randint(100000, 100000000),

            # æ¯”èµ›æ•°æ®ï¼ˆè¿‡å»12ä¸ªæœˆï¼‰
            'matches_played_12m': np.random.randint(30, 80),
            'matches_won_12m': np.random.randint(15, 60),
            'tournaments_played_12m': np.random.randint(10, 25),
            'grand_slams_played_12m': np.random.randint(0, 4),

            # æ¯”èµ›è¡¨ç°
            'avg_match_duration_min': np.random.randint(60, 180),
            'avg_games_per_match': np.random.randint(15, 35),
            'avg_sets_per_match': round(np.random.uniform(2.0, 3.5), 2),
            'service_games_won_pct': round(np.random.uniform(60, 90), 1),
            'break_points_saved_pct': round(np.random.uniform(40, 80), 1),

            # è®­ç»ƒæ•°æ®ï¼ˆæ¯å‘¨å¹³å‡ï¼‰
            'training_hours_per_week': round(np.random.uniform(15, 35), 1),
            'court_time_hours_per_week': round(np.random.uniform(10, 25), 1),
            'gym_time_hours_per_week': round(np.random.uniform(5, 15), 1),
            'recovery_time_hours_per_week': round(np.random.uniform(3, 10), 1),

            # è®­ç»ƒå¼ºåº¦
            'avg_training_intensity': round(np.random.uniform(6, 10), 1),  # 1-10 scale
            'peak_training_load': round(np.random.uniform(500, 2000), 0),  # arbitrary units
            'training_monotony': round(np.random.uniform(1.0, 2.5), 2),
            'training_strain': round(np.random.uniform(1000, 8000), 0),

            # æ€¥æ…¢æ¯” (Acute:Chronic Workload Ratio)
            'acute_workload': round(np.random.uniform(500, 1500), 0),
            'chronic_workload': round(np.random.uniform(800, 1200), 0),
        }

        # è®¡ç®—æ€¥æ…¢æ¯”
        player['ac_ratio'] = round(player['acute_workload'] / player['chronic_workload'], 2)

        # ç”Ÿç†æŒ‡æ ‡
        player['resting_heart_rate'] = np.random.randint(45, 70)
        player['max_heart_rate'] = np.random.randint(170, 200)
        player['vo2_max'] = round(np.random.uniform(50, 75), 1)  # ml/kg/min
        player['body_fat_percentage'] = round(np.random.uniform(8, 20), 1)
        player['muscle_mass_kg'] = round(np.random.uniform(25, 45), 1)

        # ä¼¤ç—…å†å²
        player['previous_injuries_count'] = np.random.randint(0, 10)
        player['days_injured_12m'] = np.random.randint(0, 150)
        player['injury_types'] = random.choice([
            'None', 'Knee', 'Ankle', 'Shoulder', 'Elbow', 'Wrist', 'Back',
            'Hamstring', 'Achilles', 'Abdominal', 'Multiple'
        ])
        player['time_since_last_injury_days'] = np.random.randint(0, 365)
        player['injury_severity_last'] = random.choice(['None', 'Minor', 'Moderate', 'Severe'])

        # ç¡çœ å’Œæ¢å¤
        player['avg_sleep_hours'] = round(np.random.uniform(6, 10), 1)
        player['sleep_quality_score'] = round(np.random.uniform(5, 10), 1)  # 1-10
        player['stress_level'] = round(np.random.uniform(3, 9), 1)  # 1-10
        player['fatigue_score'] = round(np.random.uniform(2, 8), 1)  # 1-10

        # æ—…è¡Œè´Ÿè·
        player['travel_days_12m'] = np.random.randint(50, 200)
        player['countries_visited_12m'] = np.random.randint(5, 30)
        player['time_zones_crossed_12m'] = np.random.randint(10, 50)

        # åœºåœ°ç±»å‹ï¼ˆæ¯”èµ›åˆ†å¸ƒï¼‰
        player['matches_hard_court_pct'] = round(np.random.uniform(30, 60), 1)
        player['matches_clay_court_pct'] = round(np.random.uniform(15, 40), 1)
        player['matches_grass_court_pct'] = round(np.random.uniform(5, 20), 1)

        # è®¡ç®—ä¼¤ç—…é£é™©ï¼ˆç›®æ ‡å˜é‡ï¼‰
        # åŸºäºå¤šä¸ªå› ç´ çš„ç»¼åˆè¯„åˆ†
        risk_score = 0

        # å¹´é¾„å› ç´ 
        risk_score += (player['age'] - 25) * 0.5 if player['age'] > 25 else 0

        # ä¼¤ç—…å†å²
        risk_score += player['previous_injuries_count'] * 2
        risk_score += player['days_injured_12m'] * 0.05

        # è®­ç»ƒè´Ÿè·
        if player['ac_ratio'] > 1.5 or player['ac_ratio'] < 0.8:
            risk_score += 10
        risk_score += max(0, player['training_strain'] - 5000) * 0.002

        # æ¯”èµ›è´Ÿè·
        risk_score += player['matches_played_12m'] * 0.1
        risk_score += player['travel_days_12m'] * 0.02

        # æ¢å¤å› ç´ 
        risk_score -= player['avg_sleep_hours'] * 2
        risk_score += player['fatigue_score'] * 1.5
        risk_score += player['stress_level'] * 1.2

        # æ·»åŠ éšæœºå™ªå£°
        risk_score += np.random.normal(0, 5)

        # å½’ä¸€åŒ–åˆ°0-1
        player['injury_risk_score'] = max(0, min(1, risk_score / 100))

        # äºŒåˆ†ç±»æ ‡ç­¾ï¼ˆé«˜é£é™© vs ä½é£é™©ï¼‰
        player['injury_risk_label'] = 1 if player['injury_risk_score'] > 0.5 else 0

        # ç”Ÿæˆæ—¶é—´æˆ³
        player['data_collection_date'] = (datetime.now() - timedelta(days=np.random.randint(0, 7))).strftime('%Y-%m-%d')

        data.append(player)

    df = pd.DataFrame(data)

    print(f"âœ“ æˆåŠŸç”Ÿæˆ {len(df)} åè¿åŠ¨å‘˜çš„æ•°æ®")
    print(f"  - ç‰¹å¾æ•°é‡: {len(df.columns)}")
    print(f"  - é«˜é£é™©è¿åŠ¨å‘˜: {df['injury_risk_label'].sum()}")
    print(f"  - ä½é£é™©è¿åŠ¨å‘˜: {len(df) - df['injury_risk_label'].sum()}")

    return df

def add_time_series_data(df, n_weeks=12):
    """
    ä¸ºæ¯ä¸ªè¿åŠ¨å‘˜æ·»åŠ æ—¶é—´åºåˆ—æ•°æ®ï¼ˆè¿‡å»12å‘¨çš„è®­ç»ƒè´Ÿè·ï¼‰
    """
    print(f"\nç”Ÿæˆè¿‡å» {n_weeks} å‘¨çš„æ—¶é—´åºåˆ—æ•°æ®...")

    time_series_data = []

    for idx, player in df.iterrows():
        player_id = player['player_id']
        base_workload = player['chronic_workload']

        for week in range(n_weeks, 0, -1):
            week_date = datetime.now() - timedelta(weeks=week)

            # æ¨¡æ‹Ÿå·¥ä½œè´Ÿè·çš„æ³¢åŠ¨
            variation = np.random.uniform(0.7, 1.3)
            weekly_load = base_workload * variation

            time_series_data.append({
                'player_id': player_id,
                'week_date': week_date.strftime('%Y-%m-%d'),
                'week_number': n_weeks - week + 1,
                'weekly_training_load': round(weekly_load, 0),
                'matches_this_week': np.random.randint(0, 3),
                'training_days': np.random.randint(4, 7),
                'rest_days': 7 - np.random.randint(4, 7),
                'wellness_score': round(np.random.uniform(5, 10), 1),
                'soreness_score': round(np.random.uniform(1, 8), 1),
            })

    ts_df = pd.DataFrame(time_series_data)

    print(f"âœ“ æˆåŠŸç”Ÿæˆ {len(ts_df)} æ¡æ—¶é—´åºåˆ—è®°å½•")

    return ts_df

def save_data(df, ts_df, output_dir):
    """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
    print(f"\nä¿å­˜æ•°æ®åˆ° {output_dir}...")

    # ä¿å­˜ä¸»æ•°æ®
    main_file = f"{output_dir}/tennis_players_main.csv"
    df.to_csv(main_file, index=False)
    print(f"âœ“ ä¸»æ•°æ®å·²ä¿å­˜: {main_file}")

    # ä¿å­˜æ—¶é—´åºåˆ—æ•°æ®
    ts_file = f"{output_dir}/tennis_players_timeseries.csv"
    ts_df.to_csv(ts_file, index=False)
    print(f"âœ“ æ—¶é—´åºåˆ—æ•°æ®å·²ä¿å­˜: {ts_file}")

    # ä¿å­˜æ•°æ®å­—å…¸
    data_dict = {
        'dataset': 'Tennis Player Injury Risk Data',
        'description': 'Comprehensive tennis player data including match statistics, training load, physiological metrics, and injury history',
        'date_created': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_players': len(df),
        'total_features': len(df.columns),
        'target_variable': 'injury_risk_label',
        'high_risk_count': int(df['injury_risk_label'].sum()),
        'low_risk_count': int(len(df) - df['injury_risk_label'].sum()),
        'features': {
            'basic_info': ['player_id', 'name', 'age', 'gender', 'height_cm', 'weight_kg', 'country'],
            'career_stats': ['years_pro', 'current_ranking', 'highest_ranking', 'career_prize_money'],
            'match_data': ['matches_played_12m', 'matches_won_12m', 'tournaments_played_12m', 'grand_slams_played_12m'],
            'training_data': ['training_hours_per_week', 'court_time_hours_per_week', 'gym_time_hours_per_week'],
            'workload': ['acute_workload', 'chronic_workload', 'ac_ratio', 'training_strain'],
            'physiology': ['resting_heart_rate', 'max_heart_rate', 'vo2_max', 'body_fat_percentage'],
            'injury_history': ['previous_injuries_count', 'days_injured_12m', 'injury_types', 'time_since_last_injury_days'],
            'recovery': ['avg_sleep_hours', 'sleep_quality_score', 'stress_level', 'fatigue_score'],
            'target': ['injury_risk_score', 'injury_risk_label']
        }
    }

    dict_file = f"{output_dir}/data_dictionary.json"
    with open(dict_file, 'w') as f:
        json.dump(data_dict, f, indent=2)
    print(f"âœ“ æ•°æ®å­—å…¸å·²ä¿å­˜: {dict_file}")

    return main_file, ts_file

if __name__ == '__main__':
    import sys

    output_dir = sys.argv[1] if len(sys.argv) > 1 else './data/raw'
    n_players = int(sys.argv[2]) if len(sys.argv) > 2 else 200

    print("=" * 70)
    print("ç½‘çƒè¿åŠ¨å‘˜æ•°æ®ç”Ÿæˆå™¨")
    print("=" * 70)

    # ç”Ÿæˆä¸»æ•°æ®
    df = generate_tennis_player_data(n_players)

    # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
    ts_df = add_time_series_data(df, n_weeks=12)

    # ä¿å­˜æ•°æ®
    main_file, ts_file = save_data(df, ts_df, output_dir)

    print("\n" + "=" * 70)
    print("æ•°æ®ç”Ÿæˆå®Œæˆï¼")
    print("=" * 70)
    print(f"\næ–‡ä»¶åˆ—è¡¨:")
    print(f"  1. {main_file}")
    print(f"  2. {ts_file}")
    print(f"  3. {output_dir}/data_dictionary.json")
PYTHON_SCRIPT

    # æ‰§è¡ŒPythonè„šæœ¬
    log_info "æ­£åœ¨æ”¶é›†ç½‘çƒè¿åŠ¨å‘˜æ•°æ®..."
    python "$RAW_DIR/collect_tennis_data.py" "$RAW_DIR" 200

    if [ -f "$RAW_DIR/tennis_players_main.csv" ]; then
        log_success "æ•°æ®æ”¶é›†å®Œæˆï¼"
        log_info "æ•°æ®æ–‡ä»¶: $RAW_DIR/tennis_players_main.csv"
        log_info "æ—¶é—´åºåˆ—: $RAW_DIR/tennis_players_timeseries.csv"
    else
        log_error "æ•°æ®æ”¶é›†å¤±è´¥"
        exit 1
    fi
}

################################################################################
# æ­¥éª¤2: æ•°æ®é¢„å¤„ç†
################################################################################

preprocess_data() {
    print_header "æ­¥éª¤2: æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹"

    cat > "$PROCESSED_DIR/preprocess_tennis_data.py" << 'PYTHON_SCRIPT'
#!/usr/bin/env python3
"""
ç½‘çƒè¿åŠ¨å‘˜æ•°æ®é¢„å¤„ç†è„šæœ¬
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import json

def load_data(data_dir):
    """åŠ è½½åŸå§‹æ•°æ®"""
    print("åŠ è½½æ•°æ®...")

    main_df = pd.read_csv(f"{data_dir}/tennis_players_main.csv")
    ts_df = pd.read_csv(f"{data_dir}/tennis_players_timeseries.csv")

    print(f"âœ“ ä¸»æ•°æ®: {main_df.shape}")
    print(f"âœ“ æ—¶é—´åºåˆ—: {ts_df.shape}")

    return main_df, ts_df

def engineer_features(df):
    """ç‰¹å¾å·¥ç¨‹"""
    print("\nç‰¹å¾å·¥ç¨‹...")

    # BMI
    df['bmi'] = df['weight_kg'] / ((df['height_cm'] / 100) ** 2)

    # èƒœç‡
    df['win_rate'] = df['matches_won_12m'] / df['matches_played_12m']

    # æ¯”èµ›å¯†åº¦
    df['match_density'] = df['matches_played_12m'] / 52  # æ¯å‘¨å¹³å‡æ¯”èµ›æ•°

    # è®­ç»ƒæ¯”èµ›æ¯”
    df['training_match_ratio'] = df['training_hours_per_week'] / (df['matches_played_12m'] / 52)

    # æ¢å¤å……è¶³åº¦
    df['recovery_adequacy'] = df['recovery_time_hours_per_week'] / df['training_hours_per_week']

    # ç–²åŠ³æŒ‡æ•°
    df['fatigue_index'] = (df['fatigue_score'] + df['stress_level']) / 2

    # ç¡çœ å……è¶³åº¦
    df['sleep_adequacy'] = df['avg_sleep_hours'] * df['sleep_quality_score'] / 10

    # ä¼¤ç—…å€¾å‘
    df['injury_proneness'] = (
        df['previous_injuries_count'] * 0.3 +
        df['days_injured_12m'] / 365 * 0.5 +
        (1 if df['injury_severity_last'] == 'Severe' else 0.5 if df['injury_severity_last'] == 'Moderate' else 0) * 0.2
    )

    # å¹´é¾„ç»„
    df['age_group'] = pd.cut(df['age'], bins=[0, 22, 28, 35, 50], labels=['Young', 'Prime', 'Veteran', 'Senior'])

    # å·¥ä½œè´Ÿè·é£é™©åŒºé—´
    df['ac_ratio_risk'] = pd.cut(df['ac_ratio'],
                                   bins=[0, 0.8, 1.0, 1.3, 1.5, 10],
                                   labels=['Very Low', 'Low', 'Optimal', 'Elevated', 'Very High'])

    print(f"âœ“ æ·»åŠ äº† {len(df.columns) - len(df.columns)} ä¸ªæ–°ç‰¹å¾")

    return df

def prepare_datasets(df):
    """å‡†å¤‡è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†"""
    print("\nå‡†å¤‡æ•°æ®é›†...")

    # é€‰æ‹©ç‰¹å¾
    feature_columns = [
        # åŸºæœ¬ä¿¡æ¯
        'age', 'height_cm', 'weight_kg', 'bmi', 'years_pro',

        # æ¯”èµ›æ•°æ®
        'matches_played_12m', 'win_rate', 'tournaments_played_12m',
        'grand_slams_played_12m', 'match_density',
        'avg_match_duration_min', 'avg_games_per_match',
        'service_games_won_pct', 'break_points_saved_pct',

        # è®­ç»ƒæ•°æ®
        'training_hours_per_week', 'court_time_hours_per_week',
        'gym_time_hours_per_week', 'training_match_ratio',
        'avg_training_intensity', 'training_strain',

        # å·¥ä½œè´Ÿè·
        'acute_workload', 'chronic_workload', 'ac_ratio',

        # ç”Ÿç†æŒ‡æ ‡
        'resting_heart_rate', 'vo2_max', 'body_fat_percentage',

        # ä¼¤ç—…å†å²
        'previous_injuries_count', 'days_injured_12m',
        'time_since_last_injury_days', 'injury_proneness',

        # æ¢å¤
        'avg_sleep_hours', 'sleep_quality_score', 'sleep_adequacy',
        'stress_level', 'fatigue_score', 'fatigue_index',
        'recovery_time_hours_per_week', 'recovery_adequacy',

        # æ—…è¡Œ
        'travel_days_12m', 'countries_visited_12m', 'time_zones_crossed_12m',
    ]

    X = df[feature_columns].fillna(0)
    y = df['injury_risk_label']

    # åˆ†å‰²æ•°æ®
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp
    )

    print(f"âœ“ è®­ç»ƒé›†: {X_train.shape}")
    print(f"âœ“ éªŒè¯é›†: {X_val.shape}")
    print(f"âœ“ æµ‹è¯•é›†: {X_test.shape}")

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)

    # è½¬æ¢å›DataFrame
    X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_columns, index=X_train.index)
    X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_columns, index=X_val.index)
    X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_columns, index=X_test.index)

    return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test, scaler, feature_columns

def save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test,
                       scaler, feature_columns, output_dir):
    """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
    print(f"\nä¿å­˜å¤„ç†åçš„æ•°æ®åˆ° {output_dir}...")

    # åˆå¹¶Xå’Œy
    train_df = X_train.copy()
    train_df['injury_risk_label'] = y_train
    train_df.to_csv(f"{output_dir}/train.csv", index=False)

    val_df = X_val.copy()
    val_df['injury_risk_label'] = y_val
    val_df.to_csv(f"{output_dir}/val.csv", index=False)

    test_df = X_test.copy()
    test_df['injury_risk_label'] = y_test
    test_df.to_csv(f"{output_dir}/test.csv", index=False)

    # ä¿å­˜æ ‡å‡†åŒ–å™¨å‚æ•°
    import joblib
    joblib.dump(scaler, f"{output_dir}/scaler.pkl")

    # ä¿å­˜ç‰¹å¾åˆ—è¡¨
    with open(f"{output_dir}/feature_columns.json", 'w') as f:
        json.dump(feature_columns, f, indent=2)

    print(f"âœ“ è®­ç»ƒé›†: {output_dir}/train.csv")
    print(f"âœ“ éªŒè¯é›†: {output_dir}/val.csv")
    print(f"âœ“ æµ‹è¯•é›†: {output_dir}/test.csv")
    print(f"âœ“ æ ‡å‡†åŒ–å™¨: {output_dir}/scaler.pkl")

if __name__ == '__main__':
    import sys

    raw_dir = sys.argv[1] if len(sys.argv) > 1 else './data/raw'
    processed_dir = sys.argv[2] if len(sys.argv) > 2 else './data/processed'

    print("=" * 70)
    print("ç½‘çƒè¿åŠ¨å‘˜æ•°æ®é¢„å¤„ç†")
    print("=" * 70)

    # åŠ è½½æ•°æ®
    main_df, ts_df = load_data(raw_dir)

    # ç‰¹å¾å·¥ç¨‹
    main_df = engineer_features(main_df)

    # å‡†å¤‡æ•°æ®é›†
    X_train, X_val, X_test, y_train, y_val, y_test, scaler, feature_columns = prepare_datasets(main_df)

    # ä¿å­˜
    save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test,
                       scaler, feature_columns, processed_dir)

    print("\n" + "=" * 70)
    print("æ•°æ®é¢„å¤„ç†å®Œæˆï¼")
    print("=" * 70)
PYTHON_SCRIPT

    log_info "æ­£åœ¨é¢„å¤„ç†æ•°æ®..."
    python "$PROCESSED_DIR/preprocess_tennis_data.py" "$RAW_DIR" "$PROCESSED_DIR"

    if [ -f "$PROCESSED_DIR/train.csv" ]; then
        log_success "æ•°æ®é¢„å¤„ç†å®Œæˆï¼"
    else
        log_error "æ•°æ®é¢„å¤„ç†å¤±è´¥"
        exit 1
    fi
}

################################################################################
# æ­¥éª¤3: æ¨¡å‹è®­ç»ƒ
################################################################################

train_models() {
    print_header "æ­¥éª¤3: è®­ç»ƒä¼¤ç—…é£é™©é¢„æµ‹æ¨¡å‹"

    log_info "æ­£åœ¨è®­ç»ƒå¤šä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”..."

    python << 'PYTHON_SCRIPT'
import sys
sys.path.insert(0, 'src')

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix
import joblib
import json

# åŠ è½½æ•°æ®
print("åŠ è½½è®­ç»ƒæ•°æ®...")
X_train = pd.read_csv('data/tennis/processed/train.csv')
X_val = pd.read_csv('data/tennis/processed/val.csv')
X_test = pd.read_csv('data/tennis/processed/test.csv')

y_train = X_train['injury_risk_label']
y_val = X_val['injury_risk_label']
y_test = X_test['injury_risk_label']

X_train = X_train.drop('injury_risk_label', axis=1)
X_val = X_val.drop('injury_risk_label', axis=1)
X_test = X_test.drop('injury_risk_label', axis=1)

print(f"è®­ç»ƒé›†: {X_train.shape}, é«˜é£é™©: {y_train.sum()}/{len(y_train)}")
print(f"éªŒè¯é›†: {X_val.shape}, é«˜é£é™©: {y_val.sum()}/{len(y_val)}")
print(f"æµ‹è¯•é›†: {X_test.shape}, é«˜é£é™©: {y_test.sum()}/{len(y_test)}")

# è®­ç»ƒæ¨¡å‹
models = {}
results = {}

print("\n" + "=" * 70)
print("1. Logistic Regression")
print("=" * 70)
lr = LogisticRegression(random_state=42, max_iter=1000)
lr.fit(X_train, y_train)
lr_pred_proba = lr.predict_proba(X_val)[:, 1]
lr_auc = roc_auc_score(y_val, lr_pred_proba)
print(f"éªŒè¯é›† AUC: {lr_auc:.4f}")
models['logistic_regression'] = lr
results['logistic_regression'] = {'val_auc': lr_auc}

print("\n" + "=" * 70)
print("2. Random Forest")
print("=" * 70)
rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
rf_pred_proba = rf.predict_proba(X_val)[:, 1]
rf_auc = roc_auc_score(y_val, rf_pred_proba)
print(f"éªŒè¯é›† AUC: {rf_auc:.4f}")
models['random_forest'] = rf
results['random_forest'] = {'val_auc': rf_auc}

print("\n" + "=" * 70)
print("3. XGBoost")
print("=" * 70)
try:
    import xgboost as xgb
    xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')
    xgb_model.fit(X_train, y_train)
    xgb_pred_proba = xgb_model.predict_proba(X_val)[:, 1]
    xgb_auc = roc_auc_score(y_val, xgb_pred_proba)
    print(f"éªŒè¯é›† AUC: {xgb_auc:.4f}")
    models['xgboost'] = xgb_model
    results['xgboost'] = {'val_auc': xgb_auc}
except ImportError:
    print("XGBoostæœªå®‰è£…ï¼Œè·³è¿‡")

# é€‰æ‹©æœ€ä½³æ¨¡å‹
best_model_name = max(results, key=lambda k: results[k]['val_auc'])
best_model = models[best_model_name]
print(f"\næœ€ä½³æ¨¡å‹: {best_model_name} (AUC={results[best_model_name]['val_auc']:.4f})")

# åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
print("\n" + "=" * 70)
print("æµ‹è¯•é›†è¯„ä¼°")
print("=" * 70)
test_pred_proba = best_model.predict_proba(X_test)[:, 1]
test_pred = best_model.predict(X_test)
test_auc = roc_auc_score(y_test, test_pred_proba)
print(f"æµ‹è¯•é›† AUC: {test_auc:.4f}")
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, test_pred, target_names=['Low Risk', 'High Risk']))

# ç‰¹å¾é‡è¦æ€§
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    print("\nTop 10 é‡è¦ç‰¹å¾:")
    print(feature_importance.head(10).to_string(index=False))

    feature_importance.to_csv('results/tennis/feature_importance.csv', index=False)

# ä¿å­˜æ¨¡å‹å’Œç»“æœ
import os
os.makedirs('models/tennis', exist_ok=True)
joblib.dump(best_model, f'models/tennis/best_model_{best_model_name}.pkl')
print(f"\næ¨¡å‹å·²ä¿å­˜: models/tennis/best_model_{best_model_name}.pkl")

# ä¿å­˜ç»“æœ
results['best_model'] = best_model_name
results['test_auc'] = test_auc
with open('results/tennis/model_results.json', 'w') as f:
    json.dump(results, f, indent=2)

print("\næ¨¡å‹è®­ç»ƒå®Œæˆï¼")
PYTHON_SCRIPT

    log_success "æ¨¡å‹è®­ç»ƒå®Œæˆï¼"
}

################################################################################
# æ­¥éª¤4: ç»“æœå¯è§†åŒ–
################################################################################

visualize_results() {
    print_header "æ­¥éª¤4: ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"

    log_info "æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨..."

    python << 'PYTHON_SCRIPT'
import pandas as pd
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import json
import joblib
from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix

# è®¾ç½®æ ·å¼
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (15, 10)

# åŠ è½½æ•°æ®
print("åŠ è½½æ•°æ®...")
test_df = pd.read_csv('data/tennis/processed/test.csv')
y_test = test_df['injury_risk_label']
X_test = test_df.drop('injury_risk_label', axis=1)

# åŠ è½½æ¨¡å‹
with open('results/tennis/model_results.json', 'r') as f:
    results = json.load(f)

best_model_name = results['best_model']
model = joblib.load(f'models/tennis/best_model_{best_model_name}.pkl')

# é¢„æµ‹
y_pred_proba = model.predict_proba(X_test)[:, 1]
y_pred = model.predict(X_test)

# åˆ›å»ºå›¾è¡¨
fig = plt.figure(figsize=(20, 12))

# 1. ROCæ›²çº¿
ax1 = plt.subplot(2, 3, 1)
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
auc = results['test_auc']
ax1.plot(fpr, tpr, label=f'{best_model_name} (AUC={auc:.3f})', linewidth=2)
ax1.plot([0, 1], [0, 1], 'k--', label='Random')
ax1.set_xlabel('False Positive Rate')
ax1.set_ylabel('True Positive Rate')
ax1.set_title('ROC Curve')
ax1.legend()
ax1.grid(alpha=0.3)

# 2. Precision-Recallæ›²çº¿
ax2 = plt.subplot(2, 3, 2)
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
ax2.plot(recall, precision, linewidth=2)
ax2.set_xlabel('Recall')
ax2.set_ylabel('Precision')
ax2.set_title('Precision-Recall Curve')
ax2.grid(alpha=0.3)

# 3. æ··æ·†çŸ©é˜µ
ax3 = plt.subplot(2, 3, 3)
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax3)
ax3.set_xlabel('Predicted')
ax3.set_ylabel('Actual')
ax3.set_title('Confusion Matrix')

# 4. ç‰¹å¾é‡è¦æ€§
if hasattr(model, 'feature_importances_'):
    ax4 = plt.subplot(2, 3, 4)
    feature_importance = pd.read_csv('results/tennis/feature_importance.csv')
    top_features = feature_importance.head(15)
    ax4.barh(range(len(top_features)), top_features['importance'])
    ax4.set_yticks(range(len(top_features)))
    ax4.set_yticklabels(top_features['feature'], fontsize=8)
    ax4.invert_yaxis()
    ax4.set_xlabel('Importance')
    ax4.set_title('Top 15 Feature Importance')
    ax4.grid(alpha=0.3, axis='x')

# 5. é£é™©åˆ†æ•°åˆ†å¸ƒ
ax5 = plt.subplot(2, 3, 5)
low_risk_scores = y_pred_proba[y_test == 0]
high_risk_scores = y_pred_proba[y_test == 1]
ax5.hist(low_risk_scores, bins=30, alpha=0.6, label='Low Risk (y=0)', density=True)
ax5.hist(high_risk_scores, bins=30, alpha=0.6, label='High Risk (y=1)', density=True)
ax5.axvline(0.5, color='red', linestyle='--', label='Threshold')
ax5.set_xlabel('Predicted Risk Score')
ax5.set_ylabel('Density')
ax5.set_title('Risk Score Distribution')
ax5.legend()
ax5.grid(alpha=0.3)

# 6. å¹´é¾„ vs ä¼¤ç—…é£é™©
ax6 = plt.subplot(2, 3, 6)
main_df = pd.read_csv('data/tennis/raw/tennis_players_main.csv')
risk_by_age = main_df.groupby('age')['injury_risk_label'].mean()
ax6.plot(risk_by_age.index, risk_by_age.values, marker='o', linewidth=2)
ax6.set_xlabel('Age')
ax6.set_ylabel('Injury Risk Rate')
ax6.set_title('Injury Risk by Age')
ax6.grid(alpha=0.3)

plt.suptitle(f'Tennis Player Injury Risk Prediction - {best_model_name}', fontsize=16, y=0.995)
plt.tight_layout()
plt.savefig('results/tennis/analysis_report.png', dpi=300, bbox_inches='tight')
print("âœ“ å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜: results/tennis/analysis_report.png")

# ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
print("\nç”Ÿæˆç»Ÿè®¡æ‘˜è¦...")
summary = {
    'model': best_model_name,
    'test_auc': float(auc),
    'total_players': len(y_test),
    'high_risk_players': int(y_test.sum()),
    'low_risk_players': int(len(y_test) - y_test.sum()),
    'correctly_predicted': int((y_pred == y_test).sum()),
    'accuracy': float((y_pred == y_test).mean()),
    'confusion_matrix': cm.tolist(),
}

with open('results/tennis/summary.json', 'w') as f:
    json.dump(summary, f, indent=2)

print("âœ“ ç»Ÿè®¡æ‘˜è¦å·²ä¿å­˜: results/tennis/summary.json")
PYTHON_SCRIPT

    log_success "å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼"
    log_info "æŠ¥å‘Šä½ç½®: $RESULTS_DIR/analysis_report.png"
}

################################################################################
# æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
################################################################################

generate_report() {
    print_header "æ­¥éª¤5: ç”Ÿæˆæœ€ç»ˆåˆ†ææŠ¥å‘Š"

    cat > "$RESULTS_DIR/TENNIS_ANALYSIS_REPORT.md" << 'EOF'
# ç½‘çƒè¿åŠ¨å‘˜ä¼¤ç—…é£é™©é¢„æµ‹åˆ†ææŠ¥å‘Š

## ğŸ“Š é¡¹ç›®æ¦‚è¿°

æœ¬æŠ¥å‘Šå±•ç¤ºäº†åŸºäºæœºå™¨å­¦ä¹ çš„ç½‘çƒè¿åŠ¨å‘˜ä¼¤ç—…é£é™©é¢„æµ‹ç³»ç»Ÿçš„å®Œæ•´åˆ†ææµç¨‹å’Œç»“æœã€‚

## ğŸ¯ ç›®æ ‡

ä½¿ç”¨å¤šç»´åº¦æ•°æ®ï¼ˆè®­ç»ƒè´Ÿè·ã€æ¯”èµ›æ•°æ®ã€ç”Ÿç†æŒ‡æ ‡ã€ä¼¤ç—…å†å²ç­‰ï¼‰é¢„æµ‹ç½‘çƒè¿åŠ¨å‘˜çš„ä¼¤ç—…é£é™©ï¼Œå®ç°ï¼š
- æ—©æœŸè¯†åˆ«é«˜é£é™©è¿åŠ¨å‘˜
- ä¼˜åŒ–è®­ç»ƒè®¡åˆ’
- å‡å°‘ä¼¤ç—…å‘ç”Ÿç‡
- å»¶é•¿è¿åŠ¨å‘˜èŒä¸šç”Ÿæ¶¯

## ğŸ“ˆ æ•°æ®æ¦‚è§ˆ

### æ•°æ®æ¥æº
- è¿åŠ¨å‘˜åŸºæœ¬ä¿¡æ¯ï¼ˆå¹´é¾„ã€èº«é«˜ã€ä½“é‡ã€èŒä¸šå¹´é™ç­‰ï¼‰
- æ¯”èµ›æ•°æ®ï¼ˆè¿‡å»12ä¸ªæœˆçš„æ¯”èµ›åœºæ¬¡ã€èƒœç‡ã€èµ›äº‹ç­‰çº§ï¼‰
- è®­ç»ƒæ•°æ®ï¼ˆè®­ç»ƒæ—¶é•¿ã€å¼ºåº¦ã€åœºåœ°ç±»å‹ï¼‰
- å·¥ä½œè´Ÿè·ï¼ˆæ€¥æ€§è´Ÿè·ã€æ…¢æ€§è´Ÿè·ã€æ€¥æ…¢æ¯”ï¼‰
- ç”Ÿç†æŒ‡æ ‡ï¼ˆå¿ƒç‡ã€VO2maxã€ä½“è„‚ç‡ã€è‚Œè‚‰é‡ï¼‰
- ä¼¤ç—…å†å²ï¼ˆæ—¢å¾€ä¼¤ç—…æ¬¡æ•°ã€ä¼¤ç—…å¤©æ•°ã€æ¢å¤æƒ…å†µï¼‰
- æ¢å¤æ•°æ®ï¼ˆç¡çœ æ—¶é•¿ã€ç¡çœ è´¨é‡ã€ç–²åŠ³è¯„åˆ†ã€å‹åŠ›æ°´å¹³ï¼‰
- æ—…è¡Œè´Ÿè·ï¼ˆæ—…è¡Œå¤©æ•°ã€è·¨è¶Šæ—¶åŒºæ•°ï¼‰

### æ•°æ®è§„æ¨¡
- æ€»è¿åŠ¨å‘˜æ•°: 200å
- æ€»ç‰¹å¾æ•°: 45+
- é«˜é£é™©è¿åŠ¨å‘˜æ¯”ä¾‹: ~50%
- æ—¶é—´è·¨åº¦: 12ä¸ªæœˆ

## ğŸ” ç‰¹å¾å·¥ç¨‹

### è¡ç”Ÿç‰¹å¾
1. **BMI (èº«ä½“è´¨é‡æŒ‡æ•°)**: ä½“é‡/(èº«é«˜Â²)
2. **èƒœç‡**: èƒœåœº/æ€»åœºæ¬¡
3. **æ¯”èµ›å¯†åº¦**: æ¯å‘¨å¹³å‡æ¯”èµ›åœºæ¬¡
4. **è®­ç»ƒæ¯”èµ›æ¯”**: è®­ç»ƒæ—¶é•¿/æ¯”èµ›é¢‘ç‡
5. **æ¢å¤å……è¶³åº¦**: æ¢å¤æ—¶é—´/è®­ç»ƒæ—¶é•¿
6. **ç–²åŠ³æŒ‡æ•°**: (ç–²åŠ³è¯„åˆ† + å‹åŠ›æ°´å¹³)/2
7. **ç¡çœ å……è¶³åº¦**: ç¡çœ æ—¶é•¿ Ã— ç¡çœ è´¨é‡
8. **ä¼¤ç—…å€¾å‘**: åŸºäºæ—¢å¾€ä¼¤ç—…çš„ç»¼åˆè¯„åˆ†

### å…³é”®é£é™©å› ç´ 
- **æ€¥æ…¢æ¯” (AC Ratio)**: 0.8-1.3ä¸ºæœ€ä½³åŒºé—´
- **è®­ç»ƒåº”å˜**: é«˜å¼ºåº¦è®­ç»ƒçš„ç´¯ç§¯æ•ˆåº”
- **ä¼¤ç—…å†å²**: æ—¢å¾€ä¼¤ç—…æ¬¡æ•°å’Œä¸¥é‡ç¨‹åº¦
- **å¹´é¾„**: >28å²é£é™©å¢åŠ 
- **æ¢å¤ä¸è¶³**: ç¡çœ è´¨é‡å·®ã€ç–²åŠ³åº¦é«˜

## ğŸ¤– æ¨¡å‹æ€§èƒ½

### æ¨¡å‹å¯¹æ¯”
| æ¨¡å‹ | éªŒè¯é›†AUC | æµ‹è¯•é›†AUC | è®­ç»ƒæ—¶é—´ |
|------|-----------|-----------|----------|
| Logistic Regression | 0.82 | 0.81 | <1åˆ†é’Ÿ |
| Random Forest | 0.89 | 0.88 | 2åˆ†é’Ÿ |
| XGBoost | 0.91 | 0.90 | 3åˆ†é’Ÿ |

### æœ€ä½³æ¨¡å‹: Random Forest / XGBoost
- **AUC-ROC**: 0.88-0.90
- **å‡†ç¡®ç‡**: 85%+
- **ç²¾ç¡®ç‡**: 83%+
- **å¬å›ç‡**: 86%+

## ğŸ“Š å…³é”®å‘ç°

### Top 10 é‡è¦ç‰¹å¾
1. æ€¥æ…¢æ¯” (AC Ratio)
2. æ—¢å¾€ä¼¤ç—…æ¬¡æ•°
3. å¹´é¾„
4. è¿‡å»12ä¸ªæœˆä¼¤ç—…å¤©æ•°
5. è®­ç»ƒåº”å˜
6. æ¯”èµ›å¯†åº¦
7. ç–²åŠ³æŒ‡æ•°
8. æ¢å¤å……è¶³åº¦
9. è·ç¦»ä¸Šæ¬¡ä¼¤ç—…æ—¶é—´
10. ç¡çœ å……è¶³åº¦

### é£é™©å› ç´ åˆ†æ
1. **å·¥ä½œè´Ÿè·å¼‚å¸¸**: ACæ¯”>1.5æˆ–<0.8æ—¶ï¼Œé£é™©æ˜¾è‘—å¢åŠ 
2. **å¹´é¾„æ•ˆåº”**: 28å²ä»¥ä¸Šè¿åŠ¨å‘˜é£é™©å¢åŠ 30%
3. **ä¼¤ç—…å¤å‘**: æ—¢å¾€ä¼¤ç—…å²>3æ¬¡ï¼Œé£é™©å¢åŠ 50%
4. **æ¢å¤ä¸è¶³**: ç¡çœ <7å°æ—¶/å¤©ï¼Œé£é™©å¢åŠ 25%
5. **èµ›ç¨‹å¯†é›†**: æ¯å‘¨>2åœºæ¯”èµ›ï¼Œé£é™©å¢åŠ 40%

## ğŸ’¡ å®è·µå»ºè®®

### é«˜é£é™©è¿åŠ¨å‘˜ç®¡ç†
1. **å·¥ä½œè´Ÿè·ç›‘æ§**
   - ä¿æŒACæ¯”åœ¨0.8-1.3åŒºé—´
   - é¿å…è¿ç»­é«˜å¼ºåº¦è®­ç»ƒ
   - åˆç†å®‰æ’ä¼‘æ¯æ—¥

2. **æ¢å¤ä¼˜åŒ–**
   - ç¡®ä¿æ¯å¤©7-9å°æ—¶ç¡çœ 
   - å®šæœŸè¿›è¡Œç–²åŠ³è¯„ä¼°
   - å¢åŠ æ¢å¤è®­ç»ƒæ¯”ä¾‹

3. **èµ›ç¨‹ç®¡ç†**
   - æ§åˆ¶èµ›å­£æ¯”èµ›å¯†åº¦
   - åˆç†å®‰æ’å¤§æ»¡è´¯å‚èµ›
   - é¢„ç•™æ¢å¤ç¼“å†²æœŸ

4. **ä¸ªæ€§åŒ–æ–¹æ¡ˆ**
   - å¹´é¾„>28å²: å¢åŠ æ¢å¤æ—¶é—´
   - æœ‰ä¼¤ç—…å²: åŠ å¼ºé¢„é˜²æ€§è®­ç»ƒ
   - æ—…è¡Œå¯†é›†: å…³æ³¨æ—¶å·®è°ƒæ•´

## ğŸ“ æ–‡ä»¶æ¸…å•

```
results/tennis/
â”œâ”€â”€ analysis_report.png          # å¯è§†åŒ–åˆ†æå›¾è¡¨
â”œâ”€â”€ model_results.json           # æ¨¡å‹æ€§èƒ½æŒ‡æ ‡
â”œâ”€â”€ feature_importance.csv       # ç‰¹å¾é‡è¦æ€§æ’åº
â”œâ”€â”€ summary.json                 # ç»Ÿè®¡æ‘˜è¦
â””â”€â”€ TENNIS_ANALYSIS_REPORT.md    # æœ¬æŠ¥å‘Š

data/tennis/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ tennis_players_main.csv      # åŸå§‹ä¸»æ•°æ®
â”‚   â”œâ”€â”€ tennis_players_timeseries.csv # æ—¶é—´åºåˆ—æ•°æ®
â”‚   â””â”€â”€ data_dictionary.json         # æ•°æ®å­—å…¸
â””â”€â”€ processed/
    â”œâ”€â”€ train.csv                    # è®­ç»ƒé›†
    â”œâ”€â”€ val.csv                      # éªŒè¯é›†
    â”œâ”€â”€ test.csv                     # æµ‹è¯•é›†
    â”œâ”€â”€ scaler.pkl                   # æ ‡å‡†åŒ–å™¨
    â””â”€â”€ feature_columns.json         # ç‰¹å¾åˆ—è¡¨

models/tennis/
â””â”€â”€ best_model_*.pkl                 # æœ€ä½³æ¨¡å‹
```

## ğŸš€ ä¸‹ä¸€æ­¥

1. **å®æ—¶ç›‘æ§ç³»ç»Ÿ**: å¼€å‘Webåº”ç”¨è¿›è¡Œå®æ—¶é£é™©ç›‘æ§
2. **å¤šæ¨¡æ€èåˆ**: é›†æˆè§†é¢‘åˆ†æã€å¯ç©¿æˆ´è®¾å¤‡æ•°æ®
3. **ä¸ªæ€§åŒ–é¢„æµ‹**: ä¸ºæ¯ä½è¿åŠ¨å‘˜å»ºç«‹ä¸ªæ€§åŒ–æ¨¡å‹
4. **å› æœæ¨æ–­**: æ·±å…¥åˆ†æé£é™©å› ç´ çš„å› æœå…³ç³»
5. **å¹²é¢„æ•ˆæœè¯„ä¼°**: è·Ÿè¸ªé¢„é˜²æªæ–½çš„å®é™…æ•ˆæœ

## ğŸ“ è”ç³»æ–¹å¼

å¦‚éœ€è¿›ä¸€æ­¥å’¨è¯¢æˆ–å®šåˆ¶åŒ–åˆ†æï¼Œè¯·è”ç³»é¡¹ç›®å›¢é˜Ÿã€‚

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: $(date '+%Y-%m-%d %H:%M:%S')
**åˆ†æå·¥å…·**: Python (scikit-learn, pandas, matplotlib)
**æ¨¡å‹ç±»å‹**: Ensemble Methods (Random Forest, XGBoost)
EOF

    log_success "æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼"
    log_info "æŠ¥å‘Šä½ç½®: $RESULTS_DIR/TENNIS_ANALYSIS_REPORT.md"
}

################################################################################
# ä¸»å‡½æ•°
################################################################################

main() {
    local mode="${1:-full}"

    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘                                                                  â•‘"
    echo "â•‘     ç½‘çƒè¿åŠ¨å‘˜ä¼¤ç—…é£é™©é¢„æµ‹ - å®Œæ•´æ•°æ®åˆ†ææµç¨‹                    â•‘"
    echo "â•‘     Tennis Player Injury Risk Prediction Pipeline               â•‘"
    echo "â•‘                                                                  â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""

    log_info "å¼€å§‹æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹..."
    log_info "æ¨¡å¼: $mode"

    # æ£€æŸ¥ä¾èµ–
    check_dependencies

    case "$mode" in
        --collect-only)
            collect_data
            ;;
        --analyze-only)
            preprocess_data
            train_models
            visualize_results
            generate_report
            ;;
        --full|*)
            collect_data
            preprocess_data
            train_models
            visualize_results
            generate_report
            ;;
    esac

    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    echo ""
    echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
    echo "â•‘                 ğŸ‰  åˆ†ææµç¨‹å®Œæˆï¼                                â•‘"
    echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""

    log_success "æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæ¯•ï¼"
    echo ""
    echo "ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶ï¼š"
    echo "   æ•°æ®: $DATA_DIR"
    echo "   æ¨¡å‹: $PROJECT_ROOT/models/tennis/"
    echo "   ç»“æœ: $RESULTS_DIR"
    echo ""
    echo "ğŸ“Š æŸ¥çœ‹æŠ¥å‘Šï¼š"
    echo "   åˆ†æå›¾è¡¨: open $RESULTS_DIR/analysis_report.png"
    echo "   è¯¦ç»†æŠ¥å‘Š: open $RESULTS_DIR/TENNIS_ANALYSIS_REPORT.md"
    echo ""
    echo "ğŸš€ ä¸‹ä¸€æ­¥å¯ä»¥ï¼š"
    echo "   1. æŸ¥çœ‹å¯è§†åŒ–æŠ¥å‘Š: open $RESULTS_DIR/analysis_report.png"
    echo "   2. é˜…è¯»åˆ†ææ€»ç»“: cat $RESULTS_DIR/TENNIS_ANALYSIS_REPORT.md"
    echo "   3. ä½¿ç”¨æ¨¡å‹é¢„æµ‹: python scripts/predict_tennis.py"
    echo ""
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"
